{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Parte 1: SQL y Manipulación de Datos"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-14T01:39:53.563005Z","iopub.status.busy":"2024-07-14T01:39:53.562539Z","iopub.status.idle":"2024-07-14T01:39:53.569745Z","shell.execute_reply":"2024-07-14T01:39:53.568459Z","shell.execute_reply.started":"2024-07-14T01:39:53.562971Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using Kaggle's public dataset BigQuery integration.\n"]}],"source":["from google.cloud import bigquery\n","client = bigquery.Client()\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T01:40:00.983182Z","iopub.status.busy":"2024-07-14T01:40:00.982520Z","iopub.status.idle":"2024-07-14T01:40:02.185471Z","shell.execute_reply":"2024-07-14T01:40:02.184287Z","shell.execute_reply.started":"2024-07-14T01:40:00.983144Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question_id</th>\n","      <th>titulo</th>\n","      <th>conteo_vistas</th>\n","      <th>num_respuestas</th>\n","      <th>question_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>184618</td>\n","      <td>What is the best comment in source code you ha...</td>\n","      <td>3113666</td>\n","      <td>518</td>\n","      <td>360</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>406760</td>\n","      <td>What's your most controversial programming opi...</td>\n","      <td>301961</td>\n","      <td>407</td>\n","      <td>363</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1995113</td>\n","      <td>Strangest language feature</td>\n","      <td>496025</td>\n","      <td>320</td>\n","      <td>972</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9033</td>\n","      <td>Hidden Features of C#?</td>\n","      <td>740759</td>\n","      <td>296</td>\n","      <td>1473</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1711</td>\n","      <td>What is the single most influential book every...</td>\n","      <td>1105637</td>\n","      <td>214</td>\n","      <td>1437</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   question_id                                             titulo  \\\n","0       184618  What is the best comment in source code you ha...   \n","1       406760  What's your most controversial programming opi...   \n","2      1995113                         Strangest language feature   \n","3         9033                             Hidden Features of C#?   \n","4         1711  What is the single most influential book every...   \n","\n","   conteo_vistas  num_respuestas  question_score  \n","0        3113666             518             360  \n","1         301961             407             363  \n","2         496025             320             972  \n","3         740759             296            1473  \n","4        1105637             214            1437  "]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# Preguntas más populares\n","\n","query = \"\"\"\n","SELECT\n","    q.id AS question_id\n","    , q.title AS titulo\n","    , q.view_count AS conteo_vistas\n","    , COUNT(a.id) AS num_respuestas\n","    , q.score AS question_score\n","FROM bigquery-public-data.stackoverflow.posts_questions q\n","LEFT JOIN bigquery-public-data.stackoverflow.posts_answers a ON q.id = a.parent_id\n","GROUP BY 1, 2, 3, 5\n","ORDER BY 4 DESC, 5 DESC\n","LIMIT 100;\"\"\"\n","\n","query_job = client.query(query)\n","results = query_job.to_dataframe()\n","results.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T01:40:20.053716Z","iopub.status.busy":"2024-07-14T01:40:20.053297Z","iopub.status.idle":"2024-07-14T01:40:21.052378Z","shell.execute_reply":"2024-07-14T01:40:21.051282Z","shell.execute_reply.started":"2024-07-14T01:40:20.053684Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>titulo</th>\n","      <th>conteo_vistas</th>\n","      <th>anio</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>176918</td>\n","      <td>Finding the index of an item in a list</td>\n","      <td>5582167</td>\n","      <td>2008</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>332289</td>\n","      <td>How do I change the size of figures drawn with...</td>\n","      <td>5186388</td>\n","      <td>2008</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>82831</td>\n","      <td>How do I check whether a file exists without e...</td>\n","      <td>4940426</td>\n","      <td>2008</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>363681</td>\n","      <td>How do I generate random integers within a spe...</td>\n","      <td>4849559</td>\n","      <td>2008</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>48198</td>\n","      <td>How do I find out which process is listening o...</td>\n","      <td>4796904</td>\n","      <td>2008</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id                                             titulo  conteo_vistas  \\\n","0  176918             Finding the index of an item in a list        5582167   \n","1  332289  How do I change the size of figures drawn with...        5186388   \n","2   82831  How do I check whether a file exists without e...        4940426   \n","3  363681  How do I generate random integers within a spe...        4849559   \n","4   48198  How do I find out which process is listening o...        4796904   \n","\n","   anio  \n","0  2008  \n","1  2008  \n","2  2008  \n","3  2008  \n","4  2008  "]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# Top 5 de preguntas más vistas por año\n","\n","query = \"\"\"\n","WITH top_preguntas AS (\n","    SELECT\n","        id\n","        , title AS titulo\n","        , view_count AS conteo_vistas\n","        , EXTRACT(YEAR FROM creation_date) AS anio\n","        , ROW_NUMBER() OVER (PARTITION BY EXTRACT(YEAR FROM creation_date) ORDER BY view_count DESC) AS rn\n","    FROM bigquery-public-data.stackoverflow.posts_questions\n",")\n","SELECT\n","    id\n","    , titulo\n","    , conteo_vistas\n","    , anio\n","FROM top_preguntas\n","WHERE rn <= 5\"\"\"\n","\n","query_job = client.query(query)\n","results = query_job.to_dataframe()\n","results.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T01:40:32.412091Z","iopub.status.busy":"2024-07-14T01:40:32.411634Z","iopub.status.idle":"2024-07-14T01:40:42.874196Z","shell.execute_reply":"2024-07-14T01:40:42.872840Z","shell.execute_reply.started":"2024-07-14T01:40:32.412059Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pregunta_id</th>\n","      <th>titulo_pregunta</th>\n","      <th>num_votos</th>\n","      <th>num_respuestas</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>11227809</td>\n","      <td>Why is processing a sorted array faster than p...</td>\n","      <td>38504</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>927358</td>\n","      <td>How do I undo the most recent local commits in...</td>\n","      <td>32577</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2003505</td>\n","      <td>How do I delete a Git branch locally and remot...</td>\n","      <td>25587</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>231767</td>\n","      <td>What does the \"yield\" keyword do?</td>\n","      <td>18878</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>388242</td>\n","      <td>The Definitive C++ Book Guide and List</td>\n","      <td>16117</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>6470651</td>\n","      <td>How can I create a memory leak in Java?</td>\n","      <td>5692</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>1260748</td>\n","      <td>How do I remove a submodule?</td>\n","      <td>5680</td>\n","      <td>36</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>4456438</td>\n","      <td>How to pass \"Null\" (a real surname!) to a SOAP...</td>\n","      <td>5659</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>134845</td>\n","      <td>Which \"href\" value should I use for JavaScript...</td>\n","      <td>5656</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>40471</td>\n","      <td>What are the differences between a HashMap and...</td>\n","      <td>5629</td>\n","      <td>35</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 4 columns</p>\n","</div>"],"text/plain":["    pregunta_id                                    titulo_pregunta  num_votos  \\\n","0      11227809  Why is processing a sorted array faster than p...      38504   \n","1        927358  How do I undo the most recent local commits in...      32577   \n","2       2003505  How do I delete a Git branch locally and remot...      25587   \n","3        231767                  What does the \"yield\" keyword do?      18878   \n","4        388242             The Definitive C++ Book Guide and List      16117   \n","..          ...                                                ...        ...   \n","95      6470651            How can I create a memory leak in Java?       5692   \n","96      1260748                       How do I remove a submodule?       5680   \n","97      4456438  How to pass \"Null\" (a real surname!) to a SOAP...       5659   \n","98       134845  Which \"href\" value should I use for JavaScript...       5656   \n","99        40471  What are the differences between a HashMap and...       5629   \n","\n","    num_respuestas  \n","0               27  \n","1              100  \n","2               42  \n","3               48  \n","4                1  \n","..             ...  \n","95              60  \n","96              36  \n","97               9  \n","98              55  \n","99              35  \n","\n","[100 rows x 4 columns]"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# Preguntas con más votos pero pocas respuestas\n","\n","query = \"\"\"\n","WITH preguntas_respuestas AS (\n","    SELECT\n","        q.id AS pregunta_id\n","        , q.title AS titulo_pregunta\n","        , q.creation_date AS pregunta_fecha\n","        , a.id AS respuesta_id\n","        , a.creation_date AS respuesta_fecha\n","        , a.score AS respuesta_score\n","    FROM bigquery-public-data.stackoverflow.posts_questions AS q\n","    LEFT JOIN bigquery-public-data.stackoverflow.posts_answers AS a ON q.id = a.parent_id\n","),\n","votos AS (\n","    SELECT\n","        post_id AS pregunta_id\n","        , COUNT(*) AS num_votos\n","    FROM bigquery-public-data.stackoverflow.votes\n","    GROUP BY 1\n",")\n","SELECT\n","    qa.pregunta_id\n","    , qa.titulo_pregunta\n","    , qv.num_votos\n","    , COUNT(qa.respuesta_id) AS num_respuestas\n","FROM preguntas_respuestas AS qa\n","LEFT JOIN VOTOS AS qv ON qa.pregunta_id = qv.pregunta_id\n","WHERE qv.num_votos IS NOT NULL\n","GROUP BY 1, 2, 3\n","ORDER BY qv.num_votos DESC, num_respuestas ASC\n","LIMIT 100\n","\"\"\"\n","\n","query_job = client.query(query)\n","results = query_job.to_dataframe()\n","results.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-14T02:30:04.064037Z","iopub.status.idle":"2024-07-14T02:30:04.064508Z","shell.execute_reply":"2024-07-14T02:30:04.064295Z","shell.execute_reply.started":"2024-07-14T02:30:04.064276Z"},"trusted":true},"outputs":[],"source":["# Insignias relacionadas con python\n","\n","query = \"\"\"\n","    SELECT\n","      name AS badge_name\n","      , COUNT(*) AS badge_count\n","    FROM bigquery-public-data.stackoverflow.badges\n","    WHERE tag_based = TRUE\n","    AND LOWER(name) LIKE '%python%'\n","    GROUP BY 1\n","    ORDER BY 1 DESC\n","    LIMIT 10\n","\"\"\"\n","query_job = client.query(query)\n","df = query_job.to_dataframe()\n","\n","print(df.head())\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(df['badge_name'], df['badge_count'], color='aqua')\n","plt.xlabel('Insignia')\n","plt.ylabel('Conteo')\n","plt.title(\"Insignias relacionadas con la palabra 'Python' en StackOverflow\")\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T02:36:47.740706Z","iopub.status.busy":"2024-07-14T02:36:47.740122Z","iopub.status.idle":"2024-07-14T02:36:49.018026Z","shell.execute_reply":"2024-07-14T02:36:49.016821Z","shell.execute_reply.started":"2024-07-14T02:36:47.740665Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tiempo promedio en el que una persona tarda en responder una pregunta en StackOverflow:\n","En días: 17 días, 10 horas, 42 minutos y 45 segundos\n"]}],"source":["# Tiempo promedio en el que una persona se tarda en responderte una pregunta en StackOverflow\n","\n","query = \"\"\"\n","    WITH tiempos AS (\n","        SELECT\n","          q.id AS question_id\n","          , q.creation_date AS fecha_pregunta\n","          , MIN(a.creation_date) AS primer_fecha_respuesta\n","          -- Tiempo en segundos de la fecha de respuesta y la fecha de pregunta\n","          , TIMESTAMP_DIFF(MIN(a.creation_date), q.creation_date, SECOND) AS tiempo_en_contestar\n","          , COUNT(a.id) AS num_respuestas\n","        FROM bigquery-public-data.stackoverflow.posts_questions AS q\n","        LEFT JOIN bigquery-public-data.stackoverflow.posts_answers AS a ON q.id = a.parent_id\n","        WHERE a.creation_date IS NOT NULL\n","        AND q.creation_date IS NOT NULL\n","        GROUP BY 1, 2\n","        ORDER BY 2 DESC\n","    )\n","    SELECT AVG(tiempo_en_contestar) AS promedio_tiempo_contestar\n","    FROM tiempos\n","\"\"\"\n","query_job = client.query(query)\n","\n","results = query_job.result()\n","df = results.to_dataframe()\n","\n","promedio_tiempo_contestar_segundos = df['promedio_tiempo_contestar'].iloc[0]\n","\n","#divmod regresa cociente y residuo como tupla\n","promedio_minutos, promedio_segundos = divmod(promedio_tiempo_contestar_segundos, 60)\n","promedio_horas, promedio_minutos = divmod(promedio_minutos, 60)\n","promedio_dias, promedio_horas = divmod(promedio_horas, 24)\n","\n","print(f\"Tiempo promedio en el que una persona tarda en responder una pregunta en StackOverflow:\")\n","print(f\"En días: {promedio_dias:.0f} días, {promedio_horas:.0f} horas, {promedio_minutos:.0f} minutos y {promedio_segundos:.0f} segundos\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Parte 2: Modelación de Datos y Diseño de DataWarehouse"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["2.1 Para esta sección se busca responder a las siguientes preguntas. Utiliza ejemplos concretos usando cualquier nube de tu elección:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["1. ¿Cuál es la diferencia entre Data Lake y un Data Warehouse?\n","\n","    R = Data Lake es una forma de almacenar datos estructurados o no estructurados, se almacenan datos crudos sin procesar, se utiliza cuando no tienes bien definido el análisis que harás, mientras que el Data Warehouse se almacenan datos estructurados, su acceso es más rápido, se utilizan ETL para la extracción transformación y carga de datos\n","2. ¿Cómo garantizarías que un DataWarehouse sea escalable y pueda manejar el crecimiento de los datos a lo largo del tiempo?\n","\n","    R = Definiendo particionamientos con fechas o una forma de tener la data almacenada por tiempo o identificadores\n","3. ¿Cómo transformarías datos de fuentes operacionales en un formato adecuado para análisis?\n","\n","    R = Una manera de empezar es haciendo ETL, realizar test de duplicidad, rellenar huecos en la data \n","4. ¿Cómo implementarías y asegurarías la calidad de datos y gobernanza en un Data Warehouse?\n","\n","    R = Algo que me ha funcionado es identificar siempre los datos sensibles para mantener una seguridad adecuada al momento de almacenar y disponibilizar la data. También teniendo estándares o nomenclaturas al guardar la data, tener metadatos, documentación (mapeo de datos, procedencia, es decir origen y destino de los datos)\n","5. ¿Qué buenas prácticas implementarías para asegurar la seguridad en los datos del Data Warehouse?\n","\n","    R = Esta respuesta va aunada a la anterior. Identificar datos sensibles como nombres, direcciones, biométricos, para no comprometer la seguridad de una persona. Manejar cifrado de datos, permisos, etc.\n","6. ¿Quiénes serían los usuarios de cada uno y qué tecnología utilizarías en la nube para apoyarte en tu desarrollo?\n","\n","    R = Usuarios de un Datalake podrían ser Científicos de datos e Ingenieros de Datos; Usuarios de un Data Warehouse serían analistas más enfocados al negocio o empresa, ellos utilizan datos más estructurados para toma de decisiones.\n","    Para Almacenamiento de AWS Buckes S3 y para Azure el Blob Storage\n","    Para Exploración de datos en AWS Redshift, Athena y para Azure, Data Factory para la automatización y ahora con este examen estoy aprendiendo BigQuery."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["2.2 Imaginemos que tenemos que habilitar para usuarios analíticos los datos transaccionales de las ventas a través de internet de una CPG. Describe como realizarías este proceso y cómo almacenarías las tablas de hechos y dimensiones. (Puedes tomar un ejemplo de datos existente o inventar uno)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Un ejemplo hipotética es las ventas de Coca en diferentes regiones.\n","\n","1. El primer paso es recabar y entender los requisitos del negocio. Por ejemplo realizar un análisis de las ventas por tipo de producto en las diferentes regiones.\n","2. Tener un mapeo de las fuentes de datos que se utilizarán. Se podría tener bases de datos transaccionales que detallan la info del producto, los clientes y las transacciones.\n","3. Poder identificar qué modelo de datos se va a utilizar, el que conozco es el estrella, donde existen tablas de hechos o facts y dimensiones.\n","    - Existiría una tabla que contenga la información sumarizada por fecha, región, tipo de producto y algún identificador del cliente. La columna que serviría como identificador, sería una compuesta por fecha + producto + cliente\n","    - La otra tabla dimensional podrían ser varias, alguna de ellas sería una tabla de productos que contenga: tipo de producto, costo, categoría; otra tabla dimensional sería una del tiempo que contenga: fecha, año, mes, día; otra tabla dimensional sería la de regiones que contenga: nombre de la región, CP, sensidad de población, sector, etc; y la última tabla dimensional podría ser la del cliente que contenga: tipo de cliente (atleta, no atleta), fecha de nacimiento (para obtener la edad), el ID, género etc.\n","3. Una vez teniendo facts y dimensiones, podríamos determinar el tipo de almacenamiento podría ser un Data Warehouse con datos ya estructurados, Para realizar un ETL, es decir, limpieza de datos, transformaciones\n","4. Por último, identificar qué tipos de usuarios tendrian acceso a la información, segmentar usuarios, como analistas e ingenieros de datos, y utilizar herramientas de análisis como Tableau, Looker, etc. para generar informes."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Parte 3: Procesamiento de Datos y ETL/ELT"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["3.1 Para esta sección se busca responder a las siguientes preguntas, dando ejemplos sobre alguna nube de su conocimiento"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["1. ¿Qué diferencia hay entre ETL y ELT?\n","\n","    R = En el ETL los pasos son: Extracción, Transformación y Carga, mientras que en el ELT son: Primero la Extracción, luego la Carga y al final la Transformación.\n","\n","2. ¿Cómo automatizarías los pipelines? ¿Qué criterios tomas en cuenta para elegir el tipo de automatización?\n","\n","    R = Primero tomaría en cuenta la escalabilidad, costos, facilidad de uso, monitoreo y gestión de errores.\n","\n","    - Paso 1: Diseñar el pipeline, definiéndo las etapas desde la extracción de datos, transformaciones y carga.\n","\n","    - Paso 2: Crear las tareas que realizará el pipeline, aquí se definen las ejecuciones de funciones específicas.\n","    \n","    - Paso 3: Programar la ejecución de las tareas.\n","\n","    - Paso 4: Monitorear y gestionar los errores.\n","\n","    Por ejempo si tengo que automatizar algo con AWS como herramienta podría:\n","\n","    - Paso 1: Extracción de datos en S3, programar scripts en python o pyspark de acuerdo al volumen de datos y mediante Glue crear las tareas, Cargar las tablas finales en Redshift.\n","\n","    - Paso 2: Confogurar los scripts o tareas mediante Step Functions de AWS ó airflow\n","\n","    - Paso 3: en un archivo .yaml puedo agregar la función del cron para la ejecución de tareas.\n","\n","    - Paso 4: Mediante CloudWatch se monitorean las ejecuciones.\n","\n","3. ¿Cómo gestionarías la parametrización de pipelines para que manejen distintos escenarios para no fallar ante valores hardcodeados?\n","\n","    R = Utilizando variables de entorno, por ejemplo en un archivo .env. También utilizando archivos de configuración, dentro un yaml para almacenar parámetros. Otro ejemplo podría ser los secrets de AWS para recuperar contraseñas y usuarios de forma segura.\n","\n","4. ¿Cómo gestionarías los secretos/llaves que se utilizan en los distintos pipelines, estableciendo claramente los secretos entre ambientes?\n","\n","    R = Utilizando por ejemplo AWS Secrets Manager, sé que en ese servicio se pueden aplicar políticas de acceso basado en roles. Aunque no lo he hecho, los he usado.\n","\n","5. ¿Cómo gestionarías las dependencias para asegurar que los pipelines se ejecutan en el orden correcto?\n","\n","    R = He utilizado AWS Step Functions para ello y también Airflow mediante los dags.\n","\n","6. Imagina que se tienen 1000 pipelines productivos. ¿Cómo realizarías una gestión eficiente de manejo de errores?\n","    \n","    R = Mediante alertas y notificaciones automáticas informar sobre fallos de cada pipeline, solo he utilizado Amazon CloudWatch Logs. \n","\n","7. Si se tiene una alta cantidad de pipelines, ¿Cómo realizarías el monitoreo de pipelines para ver qué establecer alertas o ser notificado en caso de fallas?\n","\n","    R = Podría crear un dashboard que centralice las alertas o fallas de los pipelines para visualizar la salud de cada uno de ellos. Hasta ahora, he utilizado Slack para mandar alertas de fallos dentro de Ariflow.\n","\n","8. ¿Qué estrategia de loggeo seguirías para diagnosticar errores, ejecuciones de pipelines, triggers, etc?\n","\n","    R = Se me ocurre incluir metadatos a los logs para facilitar la búsqueda de los errores, el id del pipeline, el timestamps de ejecución, el ambiente, etc. Tener un repositorio especial para solo logs.\n","\n","9. ¿Cómo garantizarías un versionado de los pipelines para contar con un control de cambios?\n","\n","    R = Utilizando repositorios Git ó Gitlab para el control de cambios y manejar Branch de diferentes tipos: branch para fix, features, etc.\n","\n","10. ¿Cómo establecerías en qué casos realizar loads incrementales vs full loads de manera automática?\n","    \n","    R = Incrementales cuando la data histórica no cambia y además si es un gran volumen de datos, se recomendaría realizar cargas incrementales para reducir costos de procesamiento.\n","    Full loads cuando la data es cambiante, también cuando se necesita recalcular datos o se cambia la estructura de ellos.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["3.2 Imagina que quieres ingestar datos desde una API que tiene precios de commodities. La autenticación se realiza a través de un Bearer Token.\n","\n","- ¿Para este ejemplo emplearías un proceso ETL o un proceso ELT? \n","- Diseña un diagrama de flujo para el proceso de ETL/ELT elegido y explique cada paso del proceso y su función y otras cuestiones relevantes de cómo realizarías el pipeline en la nube."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Debido a que por la naturaleza de los datos de la API, los precios pueden ser muy cambiantes o se cambian con frecuencia, se podría utilizar un proceso ELT para realizar estas extracciones rápido dentro de un ambiente raw.\n","\n","<img src=\"FlujoPipeline.png\">\n","\n","- Paso 1: Existe la API con auth Bearer Token.\n","\n","- Paso 2: En un script de python se llama a la API mediante la librería de 'requests'.\n","\n","- Paso 3: En ese mismo script del paso 2, se crea una instancia de boto3 para almacenar la data en un bucket de S3.\n","\n","- Paso 4: En un script con pyspark se crea una Spark Session, leyendo la ubicación de la data en S3 y se realizan las limpiezas y transformaciones necesarias para almacenar los datos como parquet en otra ubicación de S3.\n","\n","- Paso 5: En un script mediante BigQuery se almacena la data en una uri lista para ser consumida.\n","\n","- Paso 6: Se llama a la tabla almacenada previamente en el paso 5 desde Looker, Tableau o cualquier herramienta de BI."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Parte 4: Habilitación de datos para Analítica, visualización y reporteo"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Para esta sección supongamos que se tiene una página web de comercio electrónico de una CPG. Dado que se registran distintos eventos de la interacción de los usuarios en las páginas web, hay una volumetría muy alta. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["4.1 Se busca responder a las siguientes preguntas. Proporciona ejemplos sobre alguna nube de su conocimiento:\n","\n","- ¿Cómo prepararías y optimizarías los datos para mostrarlos eficientemente dentro de un dashboard?\n","\n","    1. Utilizaría alguna herramienta de Amazon para la ingesta de datos en tiempo real desde la página web, ó Kafka que sé que se utiliza para streaming, aunque nunca lo he usado.\n","\n","    2. Se almacenan los datos en S3\n","\n","    3. Utilizaría AWS Glue para realizar las transformaciones con Pyspark\n","\n","    4. Se almacenaría la data dentro de Redshift o Athena con datos particionados por fecha\n","\n","- ¿Cómo garantizarías la calidad e integridad de los datos antes de que se carguen en el dashboard?\n","\n","    Aplicando Data Quality, asegurando que la data venga completa y mediante Glue limpiarla, teniendo formato adecuado de fechas, enteros, cadenas, flotantes. Para el linage de datos existe AWS Lake Formation.\n","\n","- ¿Qué estrategias seguirías en conjunto con los desarrolladores de tableros para garantizar que el tablero se mantenga ágil y responda rápidamente ante la creciente volumetría?\n","        \n","    Mediante Athena podría tener consultas interactivas y más rápidas. También podría optimizar las consultas de SQL para que no hayan cargas masivas tardadas, aplicar indexaxión, particionamientos. Utilizaría vistas de las tablas también.\n","    \n","- ¿Cómo gestionarías para no mostrar datos sensibles dentro del tablero?\n","\n","    Tendríamos que configurar políticas de seguridad con AWS Identity and Access Management para restringir accesos. También puedo enmascarar la data mediante código en Glue para ocultar email, direcciones, leyendo desde el Data Catalog de AWS y haciendo transformaciones de la data para escribir los datos ya enmascarados.\n","\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["4.2 Usando el ejemplo de comercio electrónico de la CPG, ¿Qué habilitarías a nivel técnico a personas analíticos y desarrolladores de Dashboards? Explica a detalle que diseño emplearías para satisfacer las necesidades de los usuarios que permitan garantizar el rendimiento, consistencia con la información del DataWarehouse de la empresa y que tenga un tamaño adecuado a la necesidad. Explica como serían los métodos de consumo propuestos.\n","\n","1. Almacenamiento\n","\n","    - Data Lake de Amazon en S3: Aquí se almacenarían los datos crudos, provenientes de páginas web, transacciones, registros de eventos como de clicks etc.\n","    Los datos se particionarían por fecha para facilitar el acceso y consultas. Ejemplo:\n","\n","        - s3://mi-bucket/datalake/usuarios/2024/07/15/\n","        \n","        - s3://mi-bucket/datalake/transacciones/2024/07/15/\n","\n","        - s3://mi-bucket/datalake/eventos/2024/07/15/\n","    \n","    - Data Warehouse en Redshift: Se almacenan los datos transformados y optimizados para el consumo de analistas. Se utilizaría un esquema de copos de nieve con tablas de facts o hechos (transacciones, eventos) y dimensiones (usuarios, productos).\n","\n","2. Ingesta\n","\n","    - Kinesis: Crear un flujo de datos de Kinesis para capturar eventos en tiempo real, y configurarlo para almacenar la data en S3. Aquí se capturan los clicks, los views de los usuarios etc.\n","    - AWS Glue para el procesamiento y limpieza de los datos, se emplean jobs para procesar los datos y se almacenen en Redshift.\n","\n","\n","3. Catalogo\n","\n","    - AWS Glue Data Catalog donde se gestiona y se tienen los metadatos de la data que se almaceno previamente en S3 y Redshift. Aquí se definen las políticas de acceso para los usuarios. \n","\n","4. Herramientas de consumo de datos\n","    - Amazon Redshift para que los analistas ejecuten consultas en SQL, por ejemplo para generar informes mensuales de ventas, análisis sobre el comportamiento de los usuarios\n","    - Amazon Athena para desarrollo de dashboards interactivos y visualizaciones, esta herramienta es más quickSight\n","    \n","\n","<img src=\"FlujoPipeline2.png\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
